{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness' 'anger' 'love' 'surprise' 'fear' 'joy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "link = r'loc'\n",
    "shit = pd.read_csv(link, delimiter=';')\n",
    "op = shit['feeling'].unique()\n",
    "print(op)\n",
    "li = []\n",
    "for o in op:\n",
    "    p = shit.loc[shit['feeling'] == o]\n",
    "    li.append(p)\n",
    "\n",
    "for c,x in enumerate(li):\n",
    "    onehot = [one_hot(word, 16185) for word in np.array(x['statement'])]\n",
    "    li[c] = onehot\n",
    "\n",
    "Lab = shit['feeling']\n",
    "Lab = [one_hot(word, 6) for word in np.array(Lab)]\n",
    "data = np.concatenate((li[0][:500],li[1][:500],li[2][:500],li[3][:500],li[4][:500],li[5][:500]), axis=0)\n",
    "Data = []\n",
    "for c in range(500):\n",
    "    for x in range(6):\n",
    "        Data.append(data[x*500 + c])\n",
    "\n",
    "test = np.concatenate((np.full((500), 0), np.full((500), 1), np.full((500), 2), np.full((500), 3), np.full((500), 4), np.full((500), 5)))\n",
    "data.shape\n",
    "yData = []\n",
    "for c in range(500):\n",
    "    for x in range(6):\n",
    "        arr = list(np.zeros(6))\n",
    "        arr[x] = 1\n",
    "        yData.append(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 64) [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]] [[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "trainData = keras.preprocessing.sequence.pad_sequences(Data, maxlen = 64)\n",
    "lab = []\n",
    "for val in Lab:\n",
    "    arr = np.zeros(6)\n",
    "    arr[val] = 1\n",
    "    lab.append(arr)\n",
    "lab = np.array(lab)\n",
    "lab = lab.reshape(16000,6)\n",
    "yData = np.array(yData)\n",
    "print(trainData.shape, yData, lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         2071680   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         131584    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 64)          49408     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,265,286\n",
      "Trainable params: 2,265,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(16185, 128),\n",
    "    tf.keras.layers.LSTM(units = 128, activation=\"relu\", return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units = 64, activation=\"relu\", return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units = 32, activation=\"relu\", return_sequences=False),\n",
    "    tf.keras.layers.Dense(6, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2000/2000 [==============================] - 114s 56ms/step - loss: 6.0964 - accuracy: 0.6660 - val_loss: 1.2880 - val_accuracy: 0.5717\n",
      "Epoch 2/3\n",
      "1600/2000 [=======================>......] - ETA: 22s - loss: 1.8338 - accuracy: 0.7559WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 6000 batches). You may need to use the repeat() function when building your dataset.\n",
      "2000/2000 [==============================] - 91s 45ms/step - loss: 1.8338 - accuracy: 0.7559 - val_loss: 1.3302 - val_accuracy: 0.6817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19a355ad730>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\" , metrics=['accuracy'])\n",
    "model.fit(trainData, yData, epochs=3, steps_per_epoch=2000, validation_split=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['sadness' 'anger' 'love' 'surprise' 'fear' 'joy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131584"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32896 *4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  14924  1992 15109  4893]]\n",
      "[[4.3352108e-07 4.0583518e-07 9.9972516e-01 9.7155669e-07 2.4132106e-08\n",
      "  2.7287923e-04]]\n"
     ]
    }
   ],
   "source": [
    "statement = \"i am loving you\"\n",
    "statement = [np.array(one_hot(statement, 16185), dtype=\"int32\")]\n",
    "statement = keras.preprocessing.sequence.pad_sequences(statement, maxlen = 64)\n",
    "print(statement)\n",
    "print(model.predict(statement.reshape(-1, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"SmileInPain_loss=crossent.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the bias is a parameter that is always add as a parameter in the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractRNNCell',\n",
       " 'Activation',\n",
       " 'ActivityRegularization',\n",
       " 'Add',\n",
       " 'AdditiveAttention',\n",
       " 'AlphaDropout',\n",
       " 'Attention',\n",
       " 'Average',\n",
       " 'AveragePooling1D',\n",
       " 'AveragePooling2D',\n",
       " 'AveragePooling3D',\n",
       " 'AvgPool1D',\n",
       " 'AvgPool2D',\n",
       " 'AvgPool3D',\n",
       " 'BatchNormalization',\n",
       " 'Bidirectional',\n",
       " 'CategoryEncoding',\n",
       " 'CenterCrop',\n",
       " 'Concatenate',\n",
       " 'Conv1D',\n",
       " 'Conv1DTranspose',\n",
       " 'Conv2D',\n",
       " 'Conv2DTranspose',\n",
       " 'Conv3D',\n",
       " 'Conv3DTranspose',\n",
       " 'ConvLSTM1D',\n",
       " 'ConvLSTM2D',\n",
       " 'ConvLSTM3D',\n",
       " 'Convolution1D',\n",
       " 'Convolution1DTranspose',\n",
       " 'Convolution2D',\n",
       " 'Convolution2DTranspose',\n",
       " 'Convolution3D',\n",
       " 'Convolution3DTranspose',\n",
       " 'Cropping1D',\n",
       " 'Cropping2D',\n",
       " 'Cropping3D',\n",
       " 'Dense',\n",
       " 'DenseFeatures',\n",
       " 'DepthwiseConv2D',\n",
       " 'Discretization',\n",
       " 'Dot',\n",
       " 'Dropout',\n",
       " 'ELU',\n",
       " 'Embedding',\n",
       " 'Flatten',\n",
       " 'GRU',\n",
       " 'GRUCell',\n",
       " 'GaussianDropout',\n",
       " 'GaussianNoise',\n",
       " 'GlobalAveragePooling1D',\n",
       " 'GlobalAveragePooling2D',\n",
       " 'GlobalAveragePooling3D',\n",
       " 'GlobalAvgPool1D',\n",
       " 'GlobalAvgPool2D',\n",
       " 'GlobalAvgPool3D',\n",
       " 'GlobalMaxPool1D',\n",
       " 'GlobalMaxPool2D',\n",
       " 'GlobalMaxPool3D',\n",
       " 'GlobalMaxPooling1D',\n",
       " 'GlobalMaxPooling2D',\n",
       " 'GlobalMaxPooling3D',\n",
       " 'Hashing',\n",
       " 'Input',\n",
       " 'InputLayer',\n",
       " 'InputSpec',\n",
       " 'IntegerLookup',\n",
       " 'LSTM',\n",
       " 'LSTMCell',\n",
       " 'Lambda',\n",
       " 'Layer',\n",
       " 'LayerNormalization',\n",
       " 'LeakyReLU',\n",
       " 'LocallyConnected1D',\n",
       " 'LocallyConnected2D',\n",
       " 'Masking',\n",
       " 'MaxPool1D',\n",
       " 'MaxPool2D',\n",
       " 'MaxPool3D',\n",
       " 'MaxPooling1D',\n",
       " 'MaxPooling2D',\n",
       " 'MaxPooling3D',\n",
       " 'Maximum',\n",
       " 'Minimum',\n",
       " 'MultiHeadAttention',\n",
       " 'Multiply',\n",
       " 'Normalization',\n",
       " 'PReLU',\n",
       " 'Permute',\n",
       " 'RNN',\n",
       " 'RandomContrast',\n",
       " 'RandomCrop',\n",
       " 'RandomFlip',\n",
       " 'RandomHeight',\n",
       " 'RandomRotation',\n",
       " 'RandomTranslation',\n",
       " 'RandomWidth',\n",
       " 'RandomZoom',\n",
       " 'ReLU',\n",
       " 'RepeatVector',\n",
       " 'Rescaling',\n",
       " 'Reshape',\n",
       " 'Resizing',\n",
       " 'SeparableConv1D',\n",
       " 'SeparableConv2D',\n",
       " 'SeparableConvolution1D',\n",
       " 'SeparableConvolution2D',\n",
       " 'SimpleRNN',\n",
       " 'SimpleRNNCell',\n",
       " 'Softmax',\n",
       " 'SpatialDropout1D',\n",
       " 'SpatialDropout2D',\n",
       " 'SpatialDropout3D',\n",
       " 'StackedRNNCells',\n",
       " 'StringLookup',\n",
       " 'Subtract',\n",
       " 'TextVectorization',\n",
       " 'ThresholdedReLU',\n",
       " 'TimeDistributed',\n",
       " 'UpSampling1D',\n",
       " 'UpSampling2D',\n",
       " 'UpSampling3D',\n",
       " 'Wrapper',\n",
       " 'ZeroPadding1D',\n",
       " 'ZeroPadding2D',\n",
       " 'ZeroPadding3D',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'add',\n",
       " 'average',\n",
       " 'concatenate',\n",
       " 'deserialize',\n",
       " 'dot',\n",
       " 'experimental',\n",
       " 'maximum',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'serialize',\n",
       " 'subtract']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras.layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cabed6552182076907bfdc495182d8bb0133da97d0d21fa33aa63cdbe2263e8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
